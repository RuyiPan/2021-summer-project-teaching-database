---
title: "Regression analysis"
author: "Ruyi Pan"
date: "24/07/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, eval = TRUE,
  message = FALSE, warning = FALSE
)
```

```{r, echo=FALSE}
library(tidyverse)

# Modelling
library("ordinalNet")
library("ordinal")
library("numDeriv")
library("glmnet")
library("MASS")
library("VGAM")
library("glmpathcr")
library("fastDummies")
# string
library(sjmisc)
library(stringi)


library(cvms)
library(broom)    
library(tibble)   
library(ggimage)   
#> Loading required package: ggplot2
library(rsvg)   
```


# Modeling procedure

## Model 1: Fit a proportional-odds cumulative logit model

$$\text{log}\left(\frac{P(Y_i \le j)}{P( Y_i > j)}\right) =  \alpha_j + \beta^T {\bf{x}_i} \quad \mbox{for} \quad j = 1, \dots, c-1 \quad \mbox{where} \quad \frac{P(Y_i \le j)}{P( Y_i > j)} = \frac{\pi_1 + \ldots + \pi_j}{\pi_{j+1} + \ldots +\pi_{c-1}}$$.



## Model 2: Fit a continuation ratio model under the proportional odds assumptions.

$$\text{logit}(P(Y_i =j | Y_i \ge j)) =  \alpha_j + \beta^T {\bf{x}_i} \quad \mbox{for} \quad j = 1, \dots, c-1 \quad \mbox{where} \quad \omega_{ij} = P(Y_i =j | Y_i \ge j) = \frac{P(Y_i = j)}{P(Y_i \ge j)}$$.

Proportional odds: $\beta$ doesn't vary across classes, so the effect of $\beta$ is the same across $j = 1, \dots, c-1$

Continuation ratio: The continuation ratio is the relative risk of category $j$ to categories higher than $j$. It is used to account for the ordinality of the outcome. Specifically, the continuation ratio is

$$\text{logit}[P(Y_i =j | Y_i \ge j)]  = \log \left( \frac{\pi_{ij}}{\pi_{ij+1} + \dots + \pi_{ic}}  \right)$$
Interpretation of $\beta_k$: $\exp{\beta_k}$ is the relative risk ratio for category $j$ vs. categories above $j$, comparing two populations whose value of $x_k$ differs by one unit, holding the remaining variables constant. So, $\exp{\beta_k} < 1$ means that, on average, chance of being in $j$ is lower than being in a category above $j$ with increasing $x_k$ while $\exp{\beta_k} > 1$ means that, on average, chance of being in $j$ is higher than being in a category above $j$ with increasing $x_k$.

```{r}

# Slides on multinomial outcome modeling:
# http://people.vcu.edu/~dbandyop/BIOS625/chapter8.pdf
# https://cdn1.sph.harvard.edu/wp-content/uploads/sites/565/2018/08/233Spr15_Part7_Polytomous.pdf

#https://education.illinois.edu/docs/default-source/carolyn-anderson/edpsy589/lectures/8_Multicategory_logit/ordinal_logistic_post.pdf

# Metrics for Multi-Class Classification: an Overview
# https://arxiv.org/abs/2008.05756
```

```{r}
require(readxl)
data <- read_excel("data/merged_data_NR_Aug3.xlsx")
```

```{r, include=FALSE}
colnames(data)
```

```{r}
colnames(data)
```

```{r,include=FALSE}
model_data <- data %>% dplyr::select(
  Source_Type,
  Publication_Date,
  Scientific_Field,
  General_Concept,
  Specific_Concept,
  {
    "Quality (NR) (1 best)"
  },
  {
    "Statistical Sophistication (1 best)"
  }
) 
colnames(model_data) <- c(
  "Source_Type", "Date", "Scientific_Field",
  "General_Concept", "Specific_Concept", "Quality", "Sophistication"
)
```


```{r, include=FALSE}
## Extract news with labels
data1 <- model_data %>%
   filter(!is.na(Quality) & Quality != "x")
```


```{r, include=FALSE}
modified_data1 <- data1 %>%
  mutate(
    num_sepcific_concept = lengths(gregexpr(",", Specific_Concept)) + 1,
    num_general_concept = lengths(gregexpr(",", General_Concept)) + 1,
    Type = case_when(
      grepl("News", Source_Type)  ~ "News",
      grepl("Journal", Source_Type) ~ "Journal",
      grepl("Paper", Source_Type) ~ "Paper",
      grepl("Report", Source_Type) ~ "Report",
      grepl("Webpage", Source_Type) ~ "Webpage",
      T ~ "Other"
    ),
    Field = tolower(stri_extract(Scientific_Field, regex='[^,]*')),
    Concept = tolower(stri_extract(General_Concept, regex='[^,]*'))
  ) %>%
 dplyr:: select(num_sepcific_concept, 
         num_general_concept,
         Type,
         Field,
         Concept,
         Quality,
         Sophistication)
```


```{r}
data_dummies <- fastDummies::dummy_cols(modified_data1)
```

```{r}
colnames(data_dummies)
```

## Sophistication
```{r}
X <- as.matrix(data_dummies[, c(1:2, 8:38)])
Y <- as.factor(data_dummies$Sophistication)
```


```{r}
# Regularized Ordinal Regression and the ordinalNet R Package
# https://arxiv.org/abs/1706.05003

fit_cv <- try(ordinalNetCV(X, Y, family="cumulative", tuneMethod="bic",
                            link="logit", alpha=1, parallelTerms=TRUE,
                            nonparallelTerms=FALSE, standardize =T), silent=T)


fit_ONP <- ordinalNet(X, Y, family="cumulative", link="logit", alpha=1,
                   parallelTerms=TRUE, nonparallelTerms=FALSE,
                   lambdaVals=summary(fit_cv)$lambda[1])

fit_ONP
fit_ONP$coefs

res <- table(Y, predict(fit_ONP, type = 'class'))
sum(diag(res))/sum(res)
```


```{r}
# Try using aic instead -> leads to less shrinkage.
fit_cv <- try(ordinalNetCV(X, Y, family="cumulative", tuneMethod="aic",
                            link="logit", alpha=1, parallelTerms=TRUE,
                            nonparallelTerms=FALSE, standardize =T), silent=T)


fit_ONP <- ordinalNet(X, Y, family="cumulative", link="logit", alpha=1,
                   parallelTerms=TRUE, nonparallelTerms=FALSE,
                   lambdaVals=summary(fit_cv)$lambda[1])

fit_ONP
fit_ONP$coefs

res <- table(Y, predict(fit_ONP, type = 'class'))
res
sum(diag(res))/sum(res)
```



```{r}
# Check the continuation ratio model and see if its a better fit for the data.
# Continuation ratio.
# test<- vglm(model.matrix(~0+Y)~X, cratio(parallel = T))
# res_cr <- table(Y, apply(predict(test, type = 'response'), 1, which.max))


# # Proportional odds.
# data_train <- cbind(Y, X)
# fit_model <- polr(as.factor(Y)~., data = data_train)
# fit_model
# res <- table(Y, predict(fit_model, type = 'class'))
# res
# sum(diag(res))/sum(res)
# #http://hbiostat.org/papers/rms/datasetsCaseStudies/har98dev.pdf
```


```{r}
# Check linearity assumption - do note that sometimes model can indicate poor fit
# but can still predict well. 
# library('rms')
# fit_model <- lrm(Y~X, x = T, y = T)
# par(mfrow = c(3, 3))
# resid(fit_model, 'partial', pl = T) # plots cov vs partial residual
# # Linearity seems ok for spatial processing and running memory.
# 
# # Check proportional odds assumption - seems ok.
# par(mfrow = c(3, 3))
# resid(fit_model, 'score.binary', pl = T)
```

```{r}
## https://cran.r-project.org/web/packages/glmpathcr/vignettes/glmpathcr.pdf
# fit <- glmpathcr(X,Y)
# summary(fit)
# pred <- predict(fit)
# table(Y, pred)
```

## Quality

```{r}
X <- as.matrix(data_dummies[, c(1:2, 8:38)])
Y <- as.factor(data_dummies$Quality)
```

```{r}
# Try using aic instead -> leads to less shrinkage.
fit_cv <- try(ordinalNetCV(X, Y, family="cumulative", tuneMethod="bic",
                            link="logit", alpha=1, parallelTerms=TRUE,
                            nonparallelTerms=FALSE, standardize =T), silent=T)


fit_ONP <- ordinalNet(X, Y, family="cumulative", link="logit", alpha=1,
                   parallelTerms=TRUE, nonparallelTerms=FALSE,
                   lambdaVals=summary(fit_cv)$lambda[1])

fit_ONP
fit_ONP$coefs

res <- table(Y, predict(fit_ONP, type = 'class'))
res
sum(diag(res))/sum(res)
```


```{r}
# Try using aic instead -> leads to less shrinkage.
fit_cv <- try(ordinalNetCV(X, Y, family="cumulative", tuneMethod="aic",
                            link="logit", alpha=1, parallelTerms=TRUE,
                            nonparallelTerms=FALSE, standardize =T), silent=T)


fit_ONP <- ordinalNet(X, Y, family="cumulative", link="logit", alpha=1,
                   parallelTerms=TRUE, nonparallelTerms=FALSE,
                   lambdaVals=summary(fit_cv)$lambda[1])

fit_ONP
fit_ONP$coefs

res <- table(Y, predict(fit_ONP, type = 'class'))
res
sum(diag(res))/sum(res)
```
```{r}
fit_ONP$coefs
```
```{r}
colnames(data_dummies)
```

## Other packahe  CLM (Quality)
```{r}
data_dummies$num_sepcific_concept
data_dummies$num_general_concept
data_dummies$Type
data_dummies$Field
data_dummies$Concept
```

```{r}
data_dummies$Quality <- as.factor(data_dummies$Quality)
model1 <- clm(Quality ~ num_sepcific_concept + num_general_concept + Type + Field + Concept , data= data_dummies)

res <- table(data_dummies$Quality, predict(model1, type="class")$fit)
res
sum(diag(res))/sum(res)
```
```{r}
library(ggplot2)
cm_frame <- as.data.frame(res)
colnames(cm_frame) <- c("Target","Prediction","N")

cm_frame


new_frame <- cm_frame%>%mutate(Prediction=ifelse(Prediction==1,"A",ifelse(Prediction==2, "B", "C")), Target=ifelse(Target==1,"A",ifelse(Target==2, "B", "C")))

new_frame



ggplot(new_frame, mapping=aes(x=Prediction,y=Target, fill= N)) +
        geom_tile() + geom_text(aes(label = sprintf("%1.0f",N)), vjust = 1)+
        scale_fill_gradient(low="white", high="deepskyblue") +
        labs(y = "Target Quality Status",x = "Predicted Quality Status") +
        scale_x_discrete(labels=c("High","Middle","Low"),position = "top") +
        scale_y_discrete(labels=c("Low","Middle","High"),limits=rev)+
        theme(axis.text=element_text(size=12),axis.title=element_text(size=14,face="bold"),legend.position = "none")
```


```{r}
summary(model1)
chi_test <- nominal_test(model1)
```
```{r}
chi_test
```
### LRT
```{r}

model2_1 <- clm(Quality~ 1  , data= data_dummies)
model2_2 <- clm(Quality~ num_sepcific_concept  , data= data_dummies)
model2_3 <- clm(Quality~ num_sepcific_concept + num_general_concept  , data= data_dummies)
model2_4 <- clm(Quality~ num_sepcific_concept + num_general_concept + Type  , data= data_dummies)
model2_5 <- clm(Quality~ num_sepcific_concept + num_general_concept + Type + Field , data= data_dummies)
```

```{r}
anova(model2_1, model2_2)
```

```{r}

anova(model2_2, model2_3)
anova(model2_3, model2_4)
anova(model2_4, model2_5)
anova(model2_5, model1)
```



## Other package VLGM (Quality)
```{r}
model2 <- vglm(Quality ~ num_sepcific_concept + num_general_concept + Type + Field + Concept,family=cumulative(parallel=TRUE), data=data_dummies)

summary(model2)


res <- table(data_dummies$Quality, apply(predict(model2, type="response"), 1, which.max))
res
sum(diag(res))/sum(res)
```
## Other packahe  CLM (Sophistication)
```{r}
data_dummies$Sophistication <- as.factor(data_dummies$Sophistication)
model3 <- clm(Sophistication~ num_sepcific_concept + num_general_concept + Type + Field + Concept , data= data_dummies)

res3 <- table(data_dummies$Sophistication ,predict(model3, type="class")$fit)
res3
sum(diag(res3))/sum(res3)
```
```{r}
nominal_test(model3)
```
```{r}
cm_frame <- as.data.frame(res3)
colnames(cm_frame) <- c("Target","Prediction","N")

cm_frame


new_frame <- cm_frame%>%mutate(Prediction=ifelse(Prediction==1,"A",ifelse(Prediction==2, "B", "C")), Target=ifelse(Target==1,"A",ifelse(Target==2, "B", "C")))

new_frame



ggplot(new_frame, mapping=aes(x=Prediction,y=Target, fill= N)) +
        geom_tile() + geom_text(aes(label = sprintf("%1.0f",N)), vjust = 1)+
        scale_fill_gradient(low="white", high="deepskyblue") +
        labs(y = "Target Sophistication Status",x = "Predicted Sophistication Status") +
        scale_x_discrete(labels=c("High","Middle","Low"),position = "top") +
        scale_y_discrete(labels=c("Low","Middle","High"),limits=rev)+
        theme(axis.text=element_text(size=12),axis.title=element_text(size=14,face="bold"),legend.position = "none")
```

```{r}
model3_1 <- clm(Sophistication~ 1  , data= data_dummies)
model3_2 <- clm(Sophistication~ num_sepcific_concept  , data= data_dummies)
model3_3 <- clm(Sophistication~ num_sepcific_concept + num_general_concept  , data= data_dummies)
model3_4 <- clm(Sophistication~ num_sepcific_concept + num_general_concept + Type  , data= data_dummies)
model3_5 <- clm(Sophistication~ num_sepcific_concept + num_general_concept + Type + Field , data= data_dummies)
```

```{r}
anova(model3_1, model3_2)
```

```{r}

anova(model3_2, model3_3)
anova(model3_3, model3_4)
anova(model3_4, model3_5)
anova(model3_5, model3)
```

## Other packahe  VLGM (Sophistication)
```{r}
model4 <- vglm(Sophistication  ~ num_sepcific_concept + num_general_concept + Type + Field + Concept,family=cumulative(parallel=TRUE), data=data_dummies)

summary(model4)


res <- table(data_dummies$Sophistication , apply(predict(model4, type="response"), 1, which.max))
res
sum(diag(res))/sum(res)
```


